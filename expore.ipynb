{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15834 entries, 0 to 15833\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   img_path          15834 non-null  object\n",
      " 1   upscale_img_path  15834 non-null  object\n",
      " 2   label             15834 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 371.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()\n",
    "len(df['label'].unique())\n",
    "# get trained model and show wrong instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EPOCHS_50_LEARNING_RATE_0.0003_BATCH_SIZE_96_SEED_42_SPLIT_PROPORTION_0.2_BASE_ARCH_mobilenet_v3_small'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG = {\n",
    "    # 'IMG_SIZE': 224,\n",
    "    'EPOCHS': 50,\n",
    "    'LEARNING_RATE': 3e-4,\n",
    "    'BATCH_SIZE': 96,\n",
    "    'SEED': 42,\n",
    "    'SPLIT_PROPORTION': 0.2,\n",
    "    'BASE_ARCH': 'mobilenet_v3_small'\n",
    "}\n",
    "\n",
    "# convert dict to string\n",
    "CFG_STR = '_'.join([f'{k}_{v}' for k, v in CFG.items()])\n",
    "CFG_STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models\n",
    "backbone = getattr(models, 'swin_v2_s')(pretrained=True) # models.mobilenet_v3_small(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CFG = {'BASE_ARCH': 'resnet50', 'IMG_SIZE': 224}\n",
    "\n",
    "# Transformations\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "    ToTensorV2()])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "    ToTensorV2()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torchvision.transforms._presets.ImageClassification,\n",
       " ImageClassification(\n",
       "     crop_size=[224]\n",
       "     resize_size=[232]\n",
       "     mean=[0.485, 0.456, 0.406]\n",
       "     std=[0.229, 0.224, 0.225]\n",
       "     interpolation=InterpolationMode.BILINEAR\n",
       " ))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transform\n",
    "type(preprocess), preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageClassification' object has no attribute 'transforms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m test_transform \u001b[38;5;241m=\u001b[39m preprocess\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# add totensor to transform\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_transform\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mappend(ToTensorV2())\n\u001b[1;32m     20\u001b[0m train_transform\n",
      "File \u001b[0;32m~/anaconda3/envs/bird/lib/python3.11/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageClassification' object has no attribute 'transforms'"
     ]
    }
   ],
   "source": [
    "\n",
    "# torchvision.models transform\n",
    "if CFG['BASE_ARCH'] == 'resnet50':\n",
    "    weights = models.ResNet50_Weights.DEFAULT\n",
    "    preprocess = weights.transforms()\n",
    "elif CFG['BASE_ARCH'] == 'efficientnet_v2_l':\n",
    "    transforms = models.EfficientNet_V2_L_Weights.IMAGENET1K_V1.transforms\n",
    "elif CFG['BASE_ARCH'] == 'efficientnet_b7':\n",
    "    transforms = models.EfficientNet_B7_Weights.IMAGENET1K_V1.transforms\n",
    "elif CFG['BASE_ARCH'] == 'swin_v2_s':\n",
    "    transforms = models.Swin_V2_S_Weights.IMAGENET1K_V1.transforms\n",
    "\n",
    "# import pdb; pdb.set_trace()\n",
    "\n",
    "train_transform = preprocess\n",
    "test_transform = preprocess\n",
    "\n",
    "# add totensor to transform\n",
    "train_transform.transforms.append(ToTensorV2())\n",
    "train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.transforms._presets.ImageClassification"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AutoModelForImageRegression' from 'transformers' (/home/st/anaconda3/envs/bird/lib/python3.11/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoImageProcessor, AutoModelForImageRegression, TrainingArguments, Trainer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load your dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagefolder\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     data_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/your/bird/image/dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'AutoModelForImageRegression' from 'transformers' (/home/st/anaconda3/envs/bird/lib/python3.11/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoImageProcessor, AutoModelForImageRegression, TrainingArguments, Trainer\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\n",
    "    \"imagefolder\",\n",
    "    data_dir=\"path/to/your/bird/image/dataset\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "# Define the pre-trained model and processor\n",
    "model_name = \"nvidia/swinir\"\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Define the transformation function\n",
    "def transform(example_batch):\n",
    "    images = [Image.fromarray(img.convert(\"RGB\")) for img in example_batch[\"image\"]]\n",
    "    pixel_values = processor(images, return_tensors=\"pt\").pixel_values\n",
    "    return {\"pixel_values\": pixel_values}\n",
    "\n",
    "# Apply the transformation to the dataset\n",
    "transformed_dataset = dataset.with_transform(transform)\n",
    "\n",
    "# Set up the model for fine-tuning\n",
    "model = AutoModelForImageRegression.from_pretrained(model_name)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"path/to/output/directory\",\n",
    "    per_device_train_batch_size=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"psnr\",\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=transformed_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3662cbf3a1c4f1b9fafe26bb4b0bb78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/152 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a807a6b54d42fda69d7301edbd7bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad8a792537a40ac970fda005cdc6360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/48.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import AutoImageProcessor, Swin2SRForImageSuperResolution\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"caidas/swin2SR-classical-sr-x2-64\")\n",
    "model = Swin2SRForImageSuperResolution.from_pretrained(\"caidas/swin2SR-classical-sr-x2-64\")\n",
    "\n",
    "url = \"https://huggingface.co/spaces/jjourney1125/swin2sr/resolve/main/samples/butterfly.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "# prepare image for the model\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "output = outputs.reconstruction.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n",
    "output = np.moveaxis(output, source=0, destination=-1)\n",
    "output = (output * 255.0).round().astype(np.uint8)  # float32 to uint8\n",
    "# you can visualize `output` with `Image.fromarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image from test and save it to upscale_test\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from RealESRGAN import RealESRGAN\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = RealESRGAN(device, scale=4)\n",
    "model.load_weights('weights/RealESRGAN_x4.pth', download=True)\n",
    "\n",
    "indir = './test'\n",
    "outdir = './upscale_test/real_esrgan_synthetic'\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "for root, dirs, files in os.walk(indir):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg'):\n",
    "            path_to_image = os.path.join(root, file)\n",
    "            image = Image.open(path_to_image).convert('RGB')\n",
    "            sr_image = model.predict(image)\n",
    "            sr_image.save(os.path.join(outdir, file))\n",
    "            # break\n",
    "# path_to_image = '/home/st/dacon/bird_cls/train/TRAIN_00001.jpg'\n",
    "# image = Image.open(path_to_image).convert('RGB')\n",
    "\n",
    "# sr_image = model.predict(image)\n",
    "\n",
    "# sr_image.save('./sample_sr_image.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer using trained model\n",
    "from pl_baseline import BaseModel\n",
    "\n",
    "# Instantiate the model\n",
    "model = BaseModel()\n",
    "\n",
    "# load ckpt\n",
    "ckpt_path = '/home/st/dacon/bird_cls/lightning_logs/baseline_model/version_3/checkpoints/epoch=49-step=17350.ckpt'\n",
    "model = BaseModel.load_from_checkpoint(ckpt_path)\n",
    "model.eval()\n",
    "\n",
    "preds = trainer.predict(model, test_loader)\n",
    "preds = torch.cat([output for output in preds])\n",
    "# import pdb; pdb.set_trace()\n",
    "preds = le.inverse_transform(preds.argmax(1).cpu().numpy())  # Reshape preds to be 1-dimensional\n",
    "\n",
    "# Submission\n",
    "submit = pd.read_csv('./sample_submission.csv')\n",
    "submit['label'] = preds\n",
    "submit.to_csv(f'./submit_{experiment_name}.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer using image upscaler and trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/wyhuai/ddnm \n",
    "python main.py --ni --config imagenet_256_cc.yml --path_y imagenet --eta 0.85 --deg \"sr_bicubic\" --deg_scale 4.0 --sigma_y 0 -i demo\n",
    "python main.py --ni --simplified --config celeba_hq.yml --path_y solvay --eta 0.85 --deg \"sr_averagepooling\" --deg_scale 4.0 --sigma_y 0.1 -i demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use upscale model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_upscaler():\n",
    "    from pl_resnet_sr import SuperResolutionLitModule\n",
    "    ckpt_path = './lightning_logs/sr_resnet_mini_EPOCHS_100_LEARNING_RATE_0.0003_BATCH_SIZE_96_SEED_42/version_0/checkpoints/epoch=99-step=40000.ckpt'\n",
    "    upscaler = SuperResolutionLitModule()\n",
    "    upscaler.load_from_checkpoint(ckpt_path, map_location='cpu')\n",
    "    upscaler.eval()\n",
    "    return upscaler\n",
    "upscaler = load_upscaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision\n",
    "image = Image.open('./train/TRAIN_00002.jpg')\n",
    "image.size\n",
    "# to tensor\n",
    "image = torchvision.transforms.ToTensor()(image)\n",
    "# image.shape\n",
    "upscaler(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using pre-trained upsclaer and classifier model\n",
    "from pl_resnet_sr import SuperResolutionLitModule\n",
    "from pl_baseline import BaseModel\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "# Load models\n",
    "upscaler = load_upscaler()\n",
    "classifier = BaseModel.load_from_checkpoint('./lightning_logs/baseline_model/version_3/checkpoints/epoch=49-step=17350.ckpt')\n",
    "classifier.eval()\n",
    "\n",
    "# Load image\n",
    "image = Image.open('./train/TRAIN_00002.jpg')\n",
    "image = torchvision.transforms.ToTensor()(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixing #\n",
    "\n",
    "# %% \n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "# PyTorch Lightning imports\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "def load_upscaler():\n",
    "    from pl_resnet_sr import SuperResolutionLitModule\n",
    "    ckpt_path = './lightning_logs/sr_resnet_mini_EPOCHS_100_LEARNING_RATE_0.0003_BATCH_SIZE_96_SEED_42/version_0/checkpoints/epoch=99-step=40000.ckpt'\n",
    "    upscaler = SuperResolutionLitModule()\n",
    "    upscaler.load_from_checkpoint(ckpt_path, map_location='cpu')\n",
    "    upscaler.eval()\n",
    "    return upscaler\n",
    "\n",
    "# device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Hyperparameter Setting\n",
    "CFG = {\n",
    "    # 'IMG_SIZE': 224,\n",
    "    'EPOCHS': 60,\n",
    "    'LEARNING_RATE': 3e-4,\n",
    "    'BATCH_SIZE': 24, # 93\n",
    "    'SEED': 42,\n",
    "    'SPLIT_PROPORTION': 0.2,\n",
    "    'BASE_ARCH':'swin_v2_s' # efficientnet_v2_l efficientnet_b7 resnet50 swin_v2_s \n",
    "}\n",
    "\n",
    "# Fixed RandomSeed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Train & Validation Split\n",
    "df = pd.read_csv('./train.csv')\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=CFG['SPLIT_PROPORTION'], stratify=df['label'], random_state=CFG['SEED'])\n",
    "\n",
    "# Label-Encoding\n",
    "le = preprocessing.LabelEncoder()\n",
    "train['label'] = le.fit_transform(train['label'])\n",
    "val['label'] = le.transform(val['label'])\n",
    "\n",
    "# CustomDataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, transforms=None, preprocess=None):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.transforms = transforms\n",
    "        self.preprocess = preprocess\n",
    "        # \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_path_list[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        # import pdb; pdb.set_trace()\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "\n",
    "        if self.preprocess is not None:\n",
    "            \n",
    "            image = self.preprocess(image)\n",
    "\n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "\n",
    "preprocess = None\n",
    "# torchvision.models transform\n",
    "if CFG['BASE_ARCH'] == 'resnet50':\n",
    "    weights = models.ResNet50_Weights.DEFAULT\n",
    "    preprocess = weights.transforms()\n",
    "elif CFG['BASE_ARCH'] == 'efficientnet_v2_l':\n",
    "    preprocess = models.EfficientNet_V2_L_Weights.IMAGENET1K_V1.transforms()\n",
    "elif CFG['BASE_ARCH'] == 'efficientnet_b7':\n",
    "    preprocess = models.EfficientNet_B7_Weights.IMAGENET1K_V1.transforms()\n",
    "elif CFG['BASE_ARCH'] == 'swin_v2_s':\n",
    "    preprocess = models.Swin_V2_S_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "upscaler = load_upscaler()\n",
    "# Transformations\n",
    "train_transform = A.Compose([\n",
    "    # A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "    # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "    ToTensorV2(),\n",
    "    # upscaler\n",
    "    ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    # A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE']),\n",
    "    # A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
    "    ToTensorV2(),\n",
    "    # upscaler\n",
    "    ])\n",
    "\n",
    "train_dataset = CustomDataset(train['img_path'].values, train['label'].values, train_transform, preprocess=preprocess)\n",
    "train_loader = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val['img_path'].values, val['label'].values, test_transform, preprocess=preprocess)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
    "\n",
    "# Test loader\n",
    "test = pd.read_csv('./test.csv')\n",
    "# test = pd.read_csv('./test_upscale.csv')\n",
    "test_dataset = CustomDataset(test['img_path'].values, None, test_transform, preprocess=preprocess)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[256]\n",
       "    resize_size=[260]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
